{
  "id": "skill.market.agents_main.llm_application_dev.llm_evaluation.v1",
  "name": "market:skill:agents_main:llm_application_dev:llm_evaluation",
  "vendor": "agents-main",
  "status": "active",
  "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.",
  "source_path": "aura/market/catalog/skills/agents_main__llm_application_dev__llm_evaluation.md",
  "entry": {
    "type": "markdown",
    "ref": "aura/market/catalog/skills/agents_main__llm_application_dev__llm_evaluation.md"
  },
  "metadata": {
    "upstream_repo": "agents-main",
    "upstream_path": "dev/agent预备/agents-main/agents-main/plugins/llm-application-dev/skills/llm-evaluation/SKILL.md",
    "upstream_license": "MIT",
    "upstream_license_path": "aura/market/catalog/licenses/agents-main.MIT.md"
  }
}
